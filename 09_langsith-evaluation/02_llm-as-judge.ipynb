{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM As Judge\n",
    "* LLM 모델을 평가자로 설정하여 모델의 성능을 평가하고 개선할 수 있다.\n",
    "\n",
    "### OFF the shelf Evaluator\n",
    "* LandSmith에서 제공하는 기본 평가 LLM을 사용해 모델의 출력을 자동으로 평가할 수 있게 해준다.\n",
    "\n",
    "**주요특징**\n",
    "* 사전 정의된 평가 기준 제공\n",
    "* 일관된 평가 방식 적용\n",
    "* 대규모 출력 평가 자동화 기능\n",
    "\n",
    "**필요정보**\n",
    "* input: 질문, 보통 데이터셋의 Question이 사용된다.\n",
    "* prection : LLM이 생성한 답변\n",
    "* reference : 정답 답변, Context 등 변칙적으로 사용 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'백설공주는 사과를 먹고 쓰러졌습니다.'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from rag import PDFRAG\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "rag = PDFRAG(\n",
    "    file_path=\"data/snow-white.pdf\", llm=ChatOpenAI(model_name=\"gpt-4o-mini\",temperature=0)\n",
    ")\n",
    "\n",
    "retriever = rag.create_retriever()\n",
    "\n",
    "chain = rag.create_chain(retriever)\n",
    "\n",
    "chain.invoke(\"백설공주는 어떤 과일을 먹고 쓰러졌나요?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 질문에 답변하는 함수\n",
    "def ask_question(inputs : dict):\n",
    "    return {\"answer\" : chain.invoke(inputs[\"question\"])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': '백설공주는 사과를 먹고 쓰러졌습니다.'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_answer = ask_question(\n",
    "    {\"question\": \"백설공주는 어떤 과일을 먹고 쓰러졌나요?\"}\n",
    ")\n",
    "\n",
    "llm_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluator prompt 출력을 위한 함수\n",
    "def print_evaluator_prompt(evaluator):\n",
    "    return evaluator.evaluator.prompt.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question-Answer-Evaluator\n",
    "* 질문(Question)과 답변(Answer)를 평가합니다.\n",
    "\n",
    "* input: 사용자 입력\n",
    "* prediction: LLM이 생성한 답변\n",
    "* reference: 정답 답변\n",
    "\n",
    "**참고**\n",
    "Evaluator 프롬프트의 변수에는 query(input), result(prediction), answer(reference)로 정의된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a teacher grading a quiz.\n",
      "You are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\n",
      "\n",
      "Example Format:\n",
      "QUESTION: question here\n",
      "STUDENT ANSWER: student's answer here\n",
      "TRUE ANSWER: true answer here\n",
      "GRADE: CORRECT or INCORRECT here\n",
      "\n",
      "Grade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \n",
      "\n",
      "QUESTION: \u001b[33;1m\u001b[1;3m{query}\u001b[0m\n",
      "STUDENT ANSWER: \u001b[33;1m\u001b[1;3m{result}\u001b[0m\n",
      "TRUE ANSWER: \u001b[33;1m\u001b[1;3m{answer}\u001b[0m\n",
      "GRADE:\n"
     ]
    }
   ],
   "source": [
    "from langsmith.evaluation import evaluate, LangChainStringEvaluator\n",
    "\n",
    "# qa 평가자 생성\n",
    "qa_evaluator = LangChainStringEvaluator(\"qa\")\n",
    "\n",
    "print_evaluator_prompt(qa_evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"RAG_EVALUATION_DATASET\"\n",
    "\n",
    "experiement_results = evaluate(\n",
    "    ask_question, # 평가할 함수 지정\n",
    "    data=dataset_name, # 데이터셋 지정\n",
    "    evaluation=[qa_evaluator]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context에 기반한 답변 Evaluator\n",
    "**\"context_qa\"**\n",
    "* LLM 체인에 정확성을 판단하는 데 context를 사용하도록 지시\n",
    "\n",
    "**\"cot_qa\"**\n",
    "* 최종 판결을 하기 전에 LLM의 추론을 사용하도록 지시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Context를 반환하는 RAG 결과 반환 함수\n",
    "def rag_context_answer(inputs: dict):\n",
    "    context = retriever.invoke(inputs[\"question\"])\n",
    "    return {\n",
    "        \"context\": \"\\n\".join([doc.page_content for doc in context]),\n",
    "        \"answer\": chain.invoke,\n",
    "        \"query\":inputs[\"question\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#데이터셋 이름\n",
    "dataset_name = \"RAG_EVALUATION_DATASET\"\n",
    "\n",
    "# 평가실행\n",
    "evaluate(\n",
    "    rag_context_answer,\n",
    "    data=dataset_name,\n",
    "    evaluators=[cot_qa_evaluator, context_qa_evaluator],\n",
    "    experiment_prefix=\"RAG_EVALUATION\",\n",
    "    metadata={\n",
    "        \"variant\" : \"COT_QA & CONTEXT_QA Evaluatior 를 활용한 평가\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criteria\n",
    "* 기준값이 없거나 얻기 힘든 경우 \"criteria\"를 통해 사용자 지정 기준 집합에 대한 실행을 평가할 수 있다.\n",
    "* 답변에 대해 높은 수준의 의미론적 측면을 평가하고 자 할 때 유용하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangChainStringEvaluator(\"criteria\", config={ \"criteria\": `아래 중 하나의 criterion` })\n",
    "\n",
    "| 기준 | 설명 |\n",
    "|------|------|\n",
    "| `conciseness` | 답변이 간결하고 간단한지 평가 |\n",
    "| `relevance` | 답변이 질문과 관련이 있는지 평가 |\n",
    "| `correctness` | 답변이 옳은지 평가 |\n",
    "| `coherence` | 답변이 일관성이 있는지 평가 |\n",
    "| `harmfulness` | 답변이 해롭거나 유해한지 평가 |\n",
    "| `maliciousness` | 답변이 악의적이거나 악화시키는지 평가 |\n",
    "| `helpfulness` | 답변이 도움이 되는지 평가 |\n",
    "| `controversiality` | 답변이 논란이 되는지 평가 |\n",
    "| `misogyny` | 답변이 여성을 비하하는지 평가 |\n",
    "| `criminality` | 답변이 범죄를 촉진하는지 평가 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'CRITERIA_EVALUATION-765913e1' at:\n",
      "https://smith.langchain.com/o/7a65adeb-16f3-4a3a-be13-7b7aab4ae17b/datasets/917e83ee-8f0b-45b5-ba49-ed8d045141f1/compare?selectedSessions=eb7af667-01e6-4d8c-817d-fdd25e157ca0\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f092501422d4c258331a99307625bca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langsmith.evaluation import evaluate, LangChainStringEvaluator\n",
    "\n",
    "# 평가자 설정\n",
    "criteria_evaluator = [\n",
    "    LangChainStringEvaluator(\"criteria\", config={\"criteria\": \"conciseness\"}),\n",
    "    LangChainStringEvaluator(\"criteria\", config={\"criteria\": \"relevance\"}),\n",
    "    LangChainStringEvaluator(\"criteria\", config={\"criteria\": \"coherence\"})\n",
    "]\n",
    "\n",
    "# 데이터셋 이름 설정]\n",
    "dataset_name = \"RAG_EVALUATION_DATASET\"\n",
    "\n",
    "# 평가 실행\n",
    "experiment_results = evaluate(\n",
    "    ask_question,\n",
    "    data=dataset_name,\n",
    "    evaluators=criteria_evaluator,\n",
    "    experiment_prefix=\"CRITERIA_EVALUATION\",\n",
    "    metadata={\n",
    "        \"variant\": \"criteria를 활용한 평가\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
